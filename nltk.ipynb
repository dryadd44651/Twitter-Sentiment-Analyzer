{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import and Funtion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier,ConditionalExponentialClassifier,DecisionTreeClassifier,MaxentClassifier\n",
    "import re, string, random\n",
    "import pickle\n",
    "import os.path\n",
    "from os import path\n",
    "#from pattern.en import suggest\n",
    "#from autocorrect import Speller\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    #spell = Speller(lang='en')\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        token = reduce_lengthening(token)\n",
    "        #Spell Check is time consuming\n",
    "        #token = suggest(token)[0][0]#word lengthening and Spell Check\n",
    "        #try:\n",
    "        #    token = spell(token)#Spell Check\n",
    "        #except:\n",
    "        #    pass\n",
    "        \n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load positive and negative sample and create dataset<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "#     text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "#     tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "#     all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "#     freq_dist_pos = FreqDist(all_pos_words)\n",
    "#     print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the data</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "    \n",
    "#     ConditionalExponentialClassifier\n",
    "#     DecisionTreeClassifier\n",
    "#     MaxentClassifier\n",
    "#     NaiveBayesClassifier\n",
    "    \n",
    "\n",
    "    modelName = 'NaiveBayesClassifier.pickle'\n",
    "    if path.exists(modelName):\n",
    "        f = open(modelName, 'rb')\n",
    "        classifier = pickle.load(f)\n",
    "        f.close()\n",
    "    else:\n",
    "        classifier = ConditionalExponentialClassifier.train(train_data)\n",
    "        #print(classifier.show_most_informative_features(10))\n",
    "        f = open(modelName, 'wb')\n",
    "        pickle.dump(classifier, f)\n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "    #custom_tweet = \"The world’s largest animal, the blue whale, has been spotted in “unprecedented numbers” in the waters around Antarctica – a sign that the critically endangered mammal could be staging a comeback.\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "    test_data = dict([token, True] for token in custom_tokens)\n",
    "    \n",
    "    print(custom_tweet, classifier.classify(test_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "test['bad'] = True\n",
    "print(classifier.classify(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos\n",
    "custom_tweet = \"President Donald Trump approaches his first big test this week from a position of unusual weakness.\"\n",
    "#neu\n",
    "#custom_tweet = \"Trump has the lowest standing in public opinion of any new president in modern history.\"\n",
    "#neg\n",
    "#custom_tweet = \"Trump has displayed little interest in the policy itself, casting it as a thankless chore to be done before getting to tax-cut legislation he valuesmore.\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "#print(custom_tokens)\n",
    "ttt = classifier.prob_classify(dict([token, True] for token in custom_tokens))._prob_dict\n",
    "print(pow(10,ttt['Negative']),pow(10,ttt['Positive']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltkModel\n",
    "def classify(prob,range):\n",
    "    nprob = pow(10,prob['Negative'])\n",
    "    pprob = pow(10,prob['Positive'])\n",
    "    diff = pprob - nprob\n",
    "    if (nprob < range and  pprob < range) or diff < range:\n",
    "        return 'Neutral'\n",
    "    elif diff>0:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "\n",
    "\n",
    "classifier =  nltkModel.get_model('')\n",
    "\n",
    "dict_data = \"President Donald Trump approaches his first big test this week from a position of unusual weakness.\"\n",
    "test = nltkModel.get_test_dict(dict_data)\n",
    "prob = classifier.prob_classify(test)._prob_dict\n",
    "sentiment = classify(prob,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "emma_vec = Word2Vec(gutenberg.sents('austen-emma.txt'))#read nltk corpus\n",
    "emma_vec.wv.most_similar('good',  topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "# import warnings\n",
    "# warnings.filterwarnings(action = 'ignore')   \n",
    "# import gensim \n",
    "# from gensim.models import Word2Vec \n",
    "\n",
    "cleaned_tokens = positive_cleaned_tokens_list+negative_cleaned_tokens_list\n",
    "model = Word2Vec(cleaned_tokens, window=5, size= 20,min_count=1, workers=4)\n",
    "#model = Word2Vec(gutenberg.sents('austen-emma.txt'))#read nltk corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sick', 0.9933308959007263),\n",
       " ('hungry', 0.9926828742027283),\n",
       " ('bore', 0.9917027354240417),\n",
       " ('excite', 0.991385817527771),\n",
       " ('make', 0.9912464022636414),\n",
       " ('tire', 0.9910789728164673)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('bad',  topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thing', 0.9885330200195312),\n",
       " ('time', 0.9867550134658813),\n",
       " ('well', 0.9861913919448853),\n",
       " ('look', 0.9856318235397339),\n",
       " ('tire', 0.9855085611343384),\n",
       " (\"i'm\", 0.9853665828704834)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good',  topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829313"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('bad', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-167-002c05e68235>:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
      "<ipython-input-167-002c05e68235>:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n"
     ]
    }
   ],
   "source": [
    "x_traing = []\n",
    "for token in positive_cleaned_tokens_list:\n",
    "    tmp = [0 for i in range(100)]\n",
    "    for j in token:\n",
    "        tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
    "    x_traing.append(tmp)\n",
    "for token in negative_cleaned_tokens_list:\n",
    "    tmp = [0 for i in range(100)]\n",
    "    for j in token:\n",
    "        tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
    "    x_traing.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = svm.SVC()\n",
    "#clf.fit(x_traing[24000:], y_traing[24000:])\n",
    "clf.fit(x_traing, y_traing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-170-002c05e68235>:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
      "<ipython-input-170-002c05e68235>:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n"
     ]
    }
   ],
   "source": [
    "x_traing = []\n",
    "for token in positive_cleaned_tokens_list:\n",
    "    tmp = [0 for i in range(100)]\n",
    "    for j in token:\n",
    "        tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
    "    x_traing.append(tmp)\n",
    "for token in negative_cleaned_tokens_list:\n",
    "    tmp = [0 for i in range(100)]\n",
    "    for j in token:\n",
    "        tmp = list(map(lambda x,y: x+y, tmp,model[j]))\n",
    "    x_traing.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_traing= [1 if i < len(positive_cleaned_tokens_list) else 0 for i in range(len(x_traing))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_traing))\n",
    "print(len(positive_cleaned_tokens_list))\n",
    "y_traing[24990:25010]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "x_traing = numpy.asarray(x_traing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-246-a1b1478b4d49>:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  res = list(map(lambda x,y: x+y, res,model[j]))\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def word2ModelVec(tokens):\n",
    "    res = [0 for i in range(100)]\n",
    "    for j in tokens:\n",
    "        try:\n",
    "            res = list(map(lambda x,y: x+y, res,model[j]))\n",
    "        except:\n",
    "            res\n",
    "    return res\n",
    "#custom_tweet = \"good great \"\n",
    "custom_tweet = \"bad sick tire\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "token = word2ModelVec(custom_tokens)\n",
    "token = numpy.asarray(token)\n",
    "#print(clf.predict(x_traing[-5:]))\n",
    "#print(clf.predict(x_traing[0:-1]))\n",
    "\n",
    "print(clf.predict(token.reshape(-1,20)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "result = pca.fit_transform(x_traing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[5, 5], [10, 10]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "clf.predict([[8., 8.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7883407 ,  1.6349937 ,  3.5424354 ,  0.13116063,  0.5731063 ,\n",
       "         0.54748267,  0.4976517 ,  3.687261  ,  3.1158931 ,  1.35926   ],\n",
       "       [-0.75917786,  2.1996405 ,  2.176901  , -0.8459641 ,  0.34645498,\n",
       "         0.3855337 ,  1.335731  ,  2.8891735 ,  2.626511  ,  2.3379211 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "\n",
    "# What is the vector representation of the word 'graph'?\n",
    "wordvecs = model.fit(cleaned_tokens)\n",
    "wordvecs.transform(['good', 'happy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
